Project Requirements Document – RAG-Enabled Azure Chatbot (Wikipedia-Powered)

1. Introduction & Overview

Project Goal: Develop a chatbot that can answer user questions using Wikipedia as the knowledge source. The chatbot should provide accurate, fact-based responses by retrieving relevant information from Wikipedia and then generating a natural language answer. The goal is to allow users to ask questions on any topic and get a reliable answer drawn from Wikipedia content, without having to manually search Wikipedia.

Retrieval-Augmented Generation (RAG) Approach: The solution will use a Retrieval-Augmented Generation architecture. In RAG, a Large Language Model (LLM) is augmented with an information retrieval component that supplies grounding data ￼. Instead of relying solely on the LLM’s built-in knowledge (which might be outdated or prone to errors), the system first retrieves relevant documents (in this case, Wikipedia passages) and feeds them into the model to ground its responses. This approach reduces hallucinations and ensures the chatbot’s answers are supported by real content. In our project, Azure Cognitive Search will serve as the retriever and Azure OpenAI’s GPT model as the generator. This means the chatbot will search a Wikipedia index for each query, find the most relevant snippets, and then have the LLM formulate an answer using those snippets as context.

Key Technologies and Azure Services: The implementation will leverage several Azure services to achieve the above:
	•	Azure Cognitive Search (Azure AI Search) – Acts as the information retrieval system that indexes Wikipedia data and provides fast search results. It supports both keyword and vector searches, making it ideal for RAG scenarios ￼. We will use it to store and query the processed Wikipedia articles.
	•	Azure OpenAI Service – Provides the Large Language Model (e.g., GPT-3.5 or GPT-4) for generating answers. The LLM takes the user’s query and the retrieved Wikipedia content to produce a coherent answer. No extra training of the model is required; the LLM is pre-trained and uses the retrieved data as context ￼.
	•	Azure Functions – Serves as the backend orchestrator in a serverless manner. Azure Functions will host the API that the frontend calls. The function’s code will handle the end-to-end flow: receive user questions, query Azure Search, call the OpenAI API, and return the answer. This integration layer coordinates between the search index and the LLM ￼. Azure Functions was chosen for quick development and scalability (it can automatically scale out to handle more requests).
	•	Azure Cosmos DB – Used for storing chat history and any other stateful data (like user sessions or cached results). Cosmos DB will record each question and answer pair, enabling persistence of conversation context between turns. It can also store embeddings or metadata for caching purposes. Using Cosmos DB for logging Q&A pairs (with their embeddings) enables semantic look-up of past responses to improve performance and avoid redundant OpenAI calls ￼ ￼. It’s a globally distributed, highly scalable NoSQL database, so it fits the needs of storing semi-structured chat data with low latency.
	•	React Frontend – Provides the user interface (web app) where users can interact with the chatbot. The React app will allow users to input questions and will display the chatbot’s responses (possibly with reference snippets). It will communicate with the Azure Functions backend via HTTP requests (e.g., REST API calls). React was chosen for its quick development of interactive UIs and could be deployed easily as a static web app.

In summary, the project combines a web frontend with a serverless backend that orchestrates a search-augmented AI. By using Azure’s managed services (Search, OpenAI, Cosmos DB) and a RAG approach, the chatbot will deliver fact-checked answers drawn from Wikipedia content, within a small-scale architecture that can be implemented in a short timeframe.

2. Architecture Overview

High-level architecture of the RAG-enabled Azure chatbot. The user’s query travels from a React web client to an Azure Functions API, which coordinates retrieval and generation. Azure Cognitive Search (AI Search) provides relevant Wikipedia passages from its index (grounding data), and Azure OpenAI’s GPT model produces a response using that data. Azure Cosmos DB stores chat history (and potentially cached embeddings/responses). The diagram also shows optional components like API Management and caching (Redis) that can be added for security and performance in an enterprise setup, though for a basic implementation these may be simplified.

Components and Connections: The system is composed of several interconnected components, each with a distinct role:
	•	React Frontend (User Interface): The frontend is a single-page web application (SPA) built with React. It presents a chat UI to the user where they can type questions and view answers. When the user submits a query, the React app sends it to the backend (Azure Function) via an HTTP request (e.g., a POST request to a specific API endpoint). The UI will also display the conversation history, updating with each new question-answer pair. This frontend is decoupled from the backend and communicates through a well-defined API interface.
	•	Azure Functions Backend (Orchestrator): The backend consists of an Azure Function that acts as the API endpoint (for example, an HTTP-triggered function called AskQuestion). This function is the integration layer that orchestrates the RAG workflow ￼. Upon receiving a user query from the frontend, the function executes the following steps:
	1.	Query Processing: It takes the user’s question and formulates a search query. This may involve cleaning the input or appending any necessary filters.
	2.	Document Retrieval via Azure Search: The function calls Azure Cognitive Search (using the Search SDK or REST API) to retrieve relevant Wikipedia content. Depending on implementation, it might first convert the user query into an embedding vector (by calling an OpenAI embedding model) and use vector similarity search, or use the query text directly with Azure Search’s semantic capabilities. Azure Search then returns the top relevant documents or chunks (e.g., paragraphs from Wikipedia) that match the query. These search results are trimmed, concise passages meeting the token-length constraints of the LLM ￼.
	3.	LLM Response Generation: Next, the Azure Function calls the Azure OpenAI Service. It constructs a prompt for the LLM that includes the user’s question and the retrieved Wikipedia snippets as context. For example, the prompt might be a system message like: “Use the following Wikipedia information to answer the question,” followed by the snippets, and then the user’s question. The function calls the OpenAI Chat Completion or Completion API with this prompt. The OpenAI service (e.g., GPT-3.5 Turbo) generates a response.
	4.	Result Handling: The function receives the AI-generated answer. Optionally, it may post-process the answer (e.g., format citations or ensure it doesn’t exceed length limits).
	5.	Chat History Storage: The question and answer (along with maybe the retrieved document IDs or embeddings) are then stored in Azure Cosmos DB for logging and context persistence. Each entry might include a session or user ID, timestamp, the user question, the answer given, and references to source articles. Storing these in Cosmos DB allows future queries to retrieve past context or even perform semantic searches on past Q&As if needed ￼ ￼.
	6.	Response to Frontend: Finally, the Azure Function sends the answer back as an HTTP response to the React frontend. This response typically contains the answer text and possibly some metadata (like references or an answer ID).
The Azure Function connects to Azure Search using a Search API key (and endpoint URL) – these credentials are stored securely (e.g., in Azure Functions settings). It connects to Azure OpenAI using the OpenAI resource’s endpoint and API key or authentication token. For Cosmos DB, it uses a connection string or managed identity to write/read items.
	•	Azure Cognitive Search (Azure AI Search Index): This is the knowledge index built from Wikipedia data. Azure Search hosts an index that contains hundreds or thousands of short documents derived from Wikipedia articles. Each document in the index represents a chunk of a Wikipedia article (for example, a paragraph or section) and includes fields like id, title (article title), content (the chunk text), perhaps url or article_id, and a vector embedding of the content (if using vector search). Azure Search provides two key capabilities to our chatbot:
	•	Full-Text and Semantic Search: Given a user query (as text), Azure Search can perform a textual search to find relevant documents. Azure AI Search also offers semantic ranking, which can improve search result relevance by understanding context/meaning rather than just keywords.
	•	Vector Similarity Search: If we index embeddings, the service can retrieve documents by embedding similarity. We plan to use Azure OpenAI’s text-embedding model to encode each chunk of text and store that vector in the index. At query time, we encode the user query and ask Azure Search to return the closest matching vectors (passages). This ensures the retrieval is semantically relevant, not just keyword-based. Azure Search has native support for vector fields and can handle these queries efficiently ￼.
Azure Search acts as a stand-alone retrieval system that the Azure Function queries. It does not inherently know about the LLM or chat – it simply returns the most relevant wiki snippets for a given query. The index is built in advance (see Data Preparation section) and can be updated periodically if the knowledge base is expanded. The system relies on Azure Search’s millisecond-level query performance to keep the chatbot responsive. (For reference, Azure Cognitive Search is commonly used in RAG architectures for its proven reliability in indexing and querying large text corpora ￼.)
	•	Azure OpenAI Service (LLM): Azure OpenAI provides access to GPT models which generate the chatbot’s answers. In our architecture, the LLM is invoked via the Azure Function as described. The LLM’s role is to take the grounding data from Azure Search and produce a fluent answer. It receives the search results (e.g., 2-5 relevant passages) along with the original question, and is prompted to only use that information (and general knowledge) to answer. Because the model is guided by actual Wikipedia text, the response is factual and can often cite specific information. There is no fine-tuning being done on the model in this short project; we are using it in a zero-shot fashion with retrieval augmentation ￼. We will use a GPT-3.5 model for cost-efficiency unless higher accuracy is needed, in which case GPT-4 could be used (with the trade-off of higher cost and possibly slower responses). Azure OpenAI ensures the LLM service is hosted in our Azure region with enterprise-grade security and compliance.
	•	Azure Cosmos DB (State Storage): Cosmos DB is a multi-model database. We will use it with the Core (SQL) API for storing JSON documents representing chat interactions. Each chat session can have its own container or partition – for simplicity, we might use a single container (e.g., ChatHistory) partitioned by sessionId. When a user starts a chat, a session ID is created (this could be a GUID stored in the frontend or a cookie). As the user asks questions, each Q&A pair is saved to Cosmos under that session. If the user asks a follow-up question, the Azure Function can pull recent history from Cosmos to provide context to the LLM (for example, including the last user question and answer as part of the prompt to maintain continuity). Cosmos DB is chosen for its low-latency reads/writes and ability to scale throughput as needed. It also allows flexible schema – we can store additional data like embeddings of each question/answer if we want to implement semantic caching (where the system checks if a similar question was asked before). Using Cosmos in this way (to semantically cache and retrieve past responses) is a known best practice to improve performance and reduce repeated LLM calls ￼ ￼. In this initial implementation, Cosmos primarily ensures the chat experience can be stateful (multi-turn) and that we have a log of all interactions.
	•	Connections and Data Flow: The connections between components are as follows:
	•	The React app communicates with Azure Functions over HTTPS (internet). This is a simple REST call (or it could use Web Sockets for streaming, but initial implementation will use request/response HTTP).
	•	Azure Functions communicates with Azure Search using the Search REST API (over HTTPS). The function needs the search service endpoint URL and an API key for queries. This call stays within Azure’s network and is quick.
	•	Azure Functions communicates with Azure OpenAI over HTTPS as well, using the provided endpoint for the OpenAI resource and an API key or Azure AD token. (If the OpenAI service is configured with private networking, the function would need to be in the same VNet or have appropriate networking, but for a small-scale we assume public endpoint with key auth.)
	•	Azure Functions communicates with Cosmos DB using the Cosmos DB SDK or HTTPS. In Azure Functions, we can use an output binding for Cosmos to easily write documents, and an input binding or SDK call to read the last N messages for a session. This is a direct server-side call secured by the Cosmos DB primary key or a managed identity.
	•	All these connections use secure credentials: API keys for Search and OpenAI stored in app settings (not exposed to client), and Cosmos DB keys/connection strings likewise. The frontend does not directly talk to Search, OpenAI, or Cosmos – only the Azure function does, which keeps keys safe on the server side.

In essence, the architecture follows a typical RAG pipeline: user -> frontend -> orchestrator -> search -> LLM -> (store to DB) -> frontend -> user. This high-level design is illustrated by the diagram above and aligns with recommended patterns for chatbots using Azure Cognitive Search with Azure OpenAI ￼.

3. User Stories & Acceptance Criteria

To ensure the system meets end-user and developer expectations, we define several user stories with acceptance criteria:
	•	User Story 1: As a user, I want to ask the chatbot questions on various topics and receive accurate, helpful answers sourced from Wikipedia, so that I can quickly obtain information without searching manually.
Acceptance Criteria:
	1.	When the user submits a question, the system returns an answer that directly addresses the question. The answer should be factually correct and based on content from Wikipedia.
	2.	If relevant, the answer should include or reference key details (names, dates, definitions, etc.) found in the Wikipedia source. For example, if asked “Who is Alan Turing?”, the response should mention facts from Turing’s Wikipedia page (birth date, role in computing, etc.).
	3.	The response should be in clear, natural-sounding English, and be concise (a few sentences, unless a longer explanation is needed). It should not be a verbatim copy of a Wikipedia paragraph, but rather a summary or direct answer drawn from it.
	4.	If the question cannot be answered from Wikipedia (e.g., it’s off-topic or the answer isn’t in the data), the chatbot should respond gracefully with a message like “I’m sorry, I don’t know the answer to that.” It should not invent facts.
	5.	Test example: Given the question “What is Azure Cognitive Search?”, the chatbot should respond with a brief explanation that it’s a cloud search service by Microsoft Azure that provides indexing and querying over data ￼, etc., possibly mentioning it’s formerly known as Azure Search (information one would find on Wikipedia).
	•	User Story 2: As a user, I want the chatbot to leverage up-to-date Wikipedia information to ground its answers, so that I can trust the answers are based on real, current data.
Acceptance Criteria:
	1.	For each user query, the backend must perform a retrieval in the Wikipedia index. The system should fetch the top relevant document chunks (at least one, typically 3-5) from Azure Search that relate to the query.
	2.	The search results used to generate the answer should be visible for verification (either internally logged or even shown to the user as references). In a future UI enhancement, we might display the titles of Wikipedia articles used. For now, an acceptance test could be logging which Wikipedia article and section was retrieved for a given answer.
	3.	The answer content must be grounded in the retrieved data. That is, any factual statements in the answer should be traceable to the Wikipedia source. We will verify this by checking that for a set of test questions, the relevant Wikipedia snippet contains the information given in the answer. (For example, if the bot says “Alan Turing was a mathematician and logician,” those words should exist in the Wikipedia article about Alan Turing.)
	4.	The retrieval component should have acceptable performance: searching the index and returning results should take significantly less than 1 second for typical queries. (Azure Cognitive Search queries are expected to execute in tens of milliseconds for an index of this size, which meets this criterion.)
	5.	Test example: Ask “What are the uses of the element Neon?” The bot’s answer should mention uses (like in neon signage, lighting, lasers) that are verifiably from the Wikipedia article on Neon. If the bot provides such an answer and we confirm those facts exist in Wikipedia, this criterion is met. If it gave an unrelated or unsupported answer, it fails.
	•	User Story 3: As a user, I want to have a conversation with the chatbot where it remembers what I’ve asked before, so that I can ask follow-up questions without repeating context.
Acceptance Criteria:
	1.	The system should maintain chat history for the duration of a session. This means if a user asks a question, gets an answer, and then asks a related question, the chatbot can use the previous Q&A as context. The user shouldn’t need to restate information they already provided.
	2.	The backend must store each interaction (question and answer) persistently (e.g., in Cosmos DB) with a session identifier. The chat history length might be limited (for example, last 5 turns) for performance, but within that limit the context is accessible.
	3.	When the user asks a follow-up question, the Azure Function should retrieve the recent history for that session and include relevant parts of it in the prompt to Azure OpenAI. For example, if the user first asked “When was Wikipedia founded?” and then follows up with “Who founded it?”, the second query’s prompt will include the context that “Wikipedia was founded in 2001 by Jimmy Wales and Larry Sanger” (from the first answer), so that the chatbot knows “it” refers to Wikipedia and can answer “It was founded by Jimmy Wales and Larry Sanger.”
	4.	Acceptance test: Start a chat session. Ask “What is the capital of France?” (expect answer “Paris”). Then ask “How many people live there?” without saying “Paris”. The chatbot should understand “there” means Paris and use the context (population of Paris from Wikipedia) to answer correctly with a population figure. This demonstrates multi-turn memory.
	5.	The user should be able to see the conversation history in the UI (the React app will display a scrollable list of past questions and answers). This visual confirmation ensures the context is preserved. If the session is refreshed or restarted, a new conversation begins (unless we implement persistent sessions across visits, which is optional).
	•	User Story 4: As a developer/administrator, I want the system to be easy to deploy, monitor, and scale in Azure, so that it can be built in 1-2 days and run cost-efficiently for a demo.
Acceptance Criteria:
	1.	Rapid Deployment: All components can be deployed to Azure with minimal manual steps. We should be able to provision required services (Search, OpenAI, Cosmos, Function App, etc.) quickly – ideally via scripts or infrastructure-as-code. A passing criterion is the ability to deploy the entire stack in a new Azure subscription within an hour. For example, using Azure CLI or Azure Developer CLI scripts to create the services and a GitHub Actions workflow to deploy code.
	2.	Separation of Concerns: The architecture should cleanly separate the frontend and backend, and use well-defined APIs. This makes development parallelizable (frontend dev and backend dev) and the system easier to maintain. We confirm this by reviewing that the React app doesn’t contain secret keys or direct data access – it only calls the backend API. And the backend function is stateless aside from external calls (it doesn’t rely on in-memory data that would break when scaled out).
	3.	Scalability Testing: Simulate a moderate load (for example, 5 concurrent users each asking several questions). The system should handle it without errors or significant slowdown. Azure Functions on the consumption plan should auto-scale to handle bursts, Azure Search should be able to handle concurrent queries (especially if using a paid tier with sufficient throughput), and Cosmos DB in the serverless or provisioned throughput mode should handle the writes/reads. If needed, we verify by a small load test script that average response time remains good (e.g., < 2-3 seconds including LLM latency).
	4.	Cost Constraints: The solution should run within a low budget since this is a small-scale deployment. Acceptance here is qualitative – e.g., using mostly free or low-tier versions of services (Free tier of Azure Search if possible, or Basic tier; minimal throughput on Cosmos, etc.) and using the less expensive OpenAI model. We will verify that the chosen SKU of each service is the lowest that still meets functional requirements. For instance, if the index is small, Azure Cognitive Search Free (if index < 3 million documents) can be used at no cost. Cosmos DB can use the free tier or a low RU setting. OpenAI costs are pay-per-use and will be kept low by limiting the length of contexts and using GPT-3.5 for most queries.
	5.	Monitoring & Logging: As an admin, I want insight into the system’s behavior. The acceptance is that we have basic logging in place: the Azure Function should log each query and whether the pipeline succeeded (or any errors). Additionally, integration with Azure Application Insights can be enabled for the Function to track execution time and failures. Cosmos DB’s container can serve as an archive of all Q&A, which is useful for offline analysis of how the bot is performing. While full monitoring dashboards are not required in 1-2 days, having logs and the ability to evaluate them is expected. A quick check is to cause an error (e.g., ask a very long query that might overflow prompt length) and ensure the error is caught and logged by the function gracefully.

These user stories cover the core functionalities (answering questions with Wikipedia data, maintaining context, and being deployable at small scale). They guide the development to ensure the end result is user-friendly and technically sound. Each story’s acceptance criteria will be used to validate the implementation upon completion.

4. Data Sourcing & Preparation

Wikipedia Data Source: We will use Wikipedia’s content as the knowledge base for the chatbot. Wikipedia makes its content freely available through database dumps and APIs. Specifically, we have a few options to obtain the data:
	•	Wikipedia Dumps: Wikipedia offers downloadable dumps of all articles (in XML or JSON formats) on their official dumps site ￼. For example, the latest English Wikipedia dump can be downloaded (tens of GB in size). These dumps contain the full text of each article (with wikitext markup). Using a dump is suitable for bulk processing since we can get all articles at once. However, processing it can be time-consuming. Given our time constraint (1-2 days), we might opt for a smaller subset or a pre-filtered dump.
	•	Wikipedia API: Another approach is to use the MediaWiki API to fetch articles on demand. For instance, we could have a list of topics and retrieve their content via API calls. The API returns content in HTML or wikitext which we’d then clean. This approach is easier for selectively getting certain pages but not feasible for all of Wikipedia in a short time.
	•	Pre-processed Dataset: There are also publicly available datasets (on Kaggle or others) of Wikipedia content, or the output of Wikipedia in plain text format. If such a dataset (e.g., OpenQA or Wiki corpus) is available, using it could save time on parsing.

For initial implementation, we might choose a subset of Wikipedia (for example, the top 100k most viewed articles, or articles in specific categories) to reduce indexing load. If completeness is not critical, focusing on a subset can drastically cut down preparation time. In summary, the plan is either to download the Wikipedia dump and parse it, or use a pre-made dataset, ensuring we have a large set of article texts available locally.

Data Cleaning and Parsing: Once we have raw Wikipedia content, we need to clean and segment it:
	•	Remove any Wikipedia markup, templates, or irrelevant sections (like edit notes or references list) unless they contain useful info. The text should be mostly plain natural language.
	•	Strip out images, tables, or media (those won’t be used in text answers).
	•	We might also drop very short or trivial pages (disambiguation pages, etc.), focusing on content-rich pages.
	•	For each article, break it into chunks. A chunk could be a paragraph or a section. We aim for each chunk to be a self-contained piece of knowledge that is not too long. Best practice for RAG is to keep chunks small enough to fit several into the LLM prompt if needed ￼. For instance, chunks of roughly 100 to 300 words (a few hundred tokens) are a good target. If chunks are too large, we risk exceeding token limits when including multiple in the prompt; if too small, the content might lack context or meaning.
	•	A common strategy is to chunk by headings: use the article’s section structure. For example, each top-level section or subsection becomes one chunk. Another strategy is to chunk by paragraph, merging paragraphs if some are very short to meet a minimum length.
	•	Each chunk will be associated with metadata: the source article title, section title (if available), and an identifier. This metadata will be stored in the search index so we know where each chunk came from.

Indexing in Azure Cognitive Search: After preparing the text chunks, we will index them into Azure AI Search. There are two ways to do this:
	•	Push indexing: We can write a script (or Azure Function) that feeds the chunks into the Azure Search index via the Indexing API. We’d define an index schema with fields like id (key), title, content, maybe url, and content_vector (for embeddings). Then for each chunk, call the index upload API. This gives us full control to also add vector embeddings for each chunk.
	•	Indexer pipeline: Azure Cognitive Search can ingest data from external sources like Azure Blob Storage or Cosmos DB using indexers ￼. We could, for example, store all chunk JSON documents in an Azure Blob container or a Cosmos DB container, then configure Azure Search with a data source pointing to that storage and an indexer that automatically pulls in the data. This is convenient because Azure Search will handle reading the data and updating the index. If we choose this, we’ll need to set up the data source (with connection to blob or cosmos) and define the index fields mapping.

For speed, a push approach with a script might be simpler, especially if we want to include embeddings from Azure OpenAI. We will likely proceed as follows:
	•	Embedding the Chunks: Using Azure OpenAI’s Embedding model (e.g., text-embedding-ada-002), generate an embedding vector for each chunk of text. This can be done in batches. Each embedding is a high-dimensional numeric vector (say 1536 dimensions for ada-002). These will be stored in the Azure Search index (Azure Search supports vector fields for HNSW-based vector search). The embedding captures the semantic meaning of the chunk. We will also normalize or L2-normalize vectors if required by Azure Search vector config.
	•	Index Schema: Create an index in Azure Cognitive Search with these fields:
	•	id (Edm.String, key) – unique ID for each chunk, e.g., “Wikipedia_Alan_Turing_1” for first chunk of Alan Turing article.
	•	title (Edm.String, filterable/searchable) – the article title or a combined “title - section” string. Mark as searchable if we want keyword search on it (for example, if user query contains a Wikipedia article name, this helps).
	•	content (Edm.String, searchable) – the chunk text. This is the main text to full-text search on. We’ll enable full-text indexing on this.
	•	content_vector (Collection(Edm.Single), vector type) – the embedding vector for the chunk, with configuration for similarity search (likely cosine similarity). This field is set to be not searchable in the traditional sense, but used for vector similarity queries.
	•	Possibly other fields like url or section for reference (not crucial).
	•	Upload Data: Index all prepared chunks into the Search index. Ensure the index is efficiently loaded (Azure Search can index thousands of documents per minute; since Wikipedia is large, if we were indexing all of it this is a heavy job, but for a subset or given the time, we might index on the order of tens of thousands of chunks, which is doable in a couple of hours or less).

Best Practices for Indexing: We will apply these practices to optimize the RAG performance:
	•	Chunk Sizing: Aim for chunks that capture a complete thought or paragraph. As noted, a few hundred words per chunk is ideal for balancing context and prompt length. This aligns with Azure Search’s recommendation to return results in short form relevant snippets ￼. Short snippets ensure that when we pass them to the LLM, we can include multiple and still stay within token limits.
	•	Overlapping Chunks (if needed): In some cases, to avoid losing context between paragraphs, one can overlap chunks slightly. Given time constraints, we may or may not do this. If an article has very long sections, we might break them with some overlap so that important info at boundaries isn’t lost.
	•	Filtering Out Noise: We won’t index lists of references, navigation menus, or irrelevant text. This keeps the index focused and improves search quality. We might also exclude extremely short stub articles that don’t provide meaningful content.
	•	Testing the Index: After indexing, we will run a few sample queries directly against Azure Search (via the portal or SDK) to ensure it returns relevant passages. For example, test search for “Alan Turing computer science” and see that it returns a chunk from the Alan Turing article. Fine-tuning ranking (e.g., using Azure Search’s scoring profiles or semantic ranking) is probably not needed for a basic setup but is an option if results are not as expected.
	•	Refresh Strategy: Wikipedia content changes over time. In a production system, we’d plan how to update the index (perhaps re-run the ingestion periodically or use Wikipedia’s update feeds). For our short-term project, a static snapshot is fine. We note that we’re using (for now) a fixed dump of Wikipedia data, so it might not include the very latest edits. This is acceptable for the demo.

Once this data preparation is done, we will have an Azure Search index populated with Wikipedia knowledge. That index is the backbone of the RAG system, enabling the subsequent steps of retrieval and answer generation.

5. Service Integration & Interfaces

This section describes the interfaces between components and how the services will be integrated at a high level. We focus on API design and connection points without going into code.

Frontend–Backend API Design: The React frontend will communicate with the Azure Functions backend via a RESTful HTTP API. We will design a simple JSON-based protocol. For example:
	•	Endpoint: POST /api/ask (hosted by Azure Functions, e.g., https://<function-app-url>/api/ask). This single endpoint can handle all chat queries.
	•	Request Format: JSON object containing the user’s query and some metadata. Fields might include:
	•	question (string) – the user’s question text.
	•	sessionId (string) – an identifier for the chat session (to retrieve context). This could be generated by the frontend for each new chat or returned by backend on first call. If not provided (first question), the backend generates a new session ID.
	•	Optionally, history or context – the frontend might send the recent conversation context, but since we store it server-side (Cosmos DB), this might not be needed. Simpler design: just send sessionId and question, and let server pull history.
	•	Response Format: JSON object containing the chatbot’s answer and any additional info. Fields:
	•	answer (string) – the answer text generated by the AI.
	•	sources (array of objects, optional) – a list of source data used. For example, this could include the titles of Wikipedia articles or URLs that were retrieved. This can be used by the UI to display “Sources: Wikipedia Article X, Wikipedia Article Y” or for debugging. Each source entry might have title and maybe snippet or url.
	•	sessionId (string) – if a new session was created, return its ID (so the frontend can use it in subsequent calls).
	•	error (string, optional) – if something went wrong, an error message (the frontend can display a generic “Sorry, an error occurred”).

Example request JSON:

{
  "sessionId": "abcd-1234-efgh-5678",
  "question": "Who discovered penicillin?"
}

Example response JSON:

{
  "answer": "Penicillin was discovered in 1928 by Scottish scientist Alexander Fleming. [oai_citation_attribution:21‡learn.microsoft.com](https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview#:~:text=The%20web%20app%20provides%20the,set%20the%20context%20and%20intent)",
  "sources": [
    { "title": "Penicillin - Wikipedia", "url": "https://en.wikipedia.org/wiki/Penicillin" }
  ],
  "sessionId": "abcd-1234-efgh-5678"
}

(The citation above is just illustrative – in an actual answer, the model’s text might not include a citation bracket, but we could choose to include Wikipedia as references if formatting answers that way.)

The API is stateless regarding HTTP (each call contains needed info like sessionId), but stateful in the sense that the server uses the sessionId to fetch past context.

Backend Integration (Azure Function): Within the Azure Function (backend), integration with Azure services occurs as follows:
	•	Azure Search Query: The function will use the Azure Search SDK or REST call to search the index. If using vector search, the function must first call Azure OpenAI’s embedding API. That would be a REST call to the OpenAI embedding endpoint with the user question. The result is an embedding vector (likely 1536 floats). Then the function calls the Azure Search REST API’s query endpoint, including the vector (in base64 or JSON depending on API) and specifying parameters like top k=3 results, similarity function, etc. If using keyword/semantic search instead, it would send the query as a simple keyword string (or use semantic query via Azure Cognitive Search REST by setting queryType=semantic and perhaps queryLanguage=en). We will decide on one approach and implement it. The result from Azure Search comes in JSON, with fields for each hit (like content, score, title, etc. and possibly the vector score if vector search).
	•	Azure OpenAI API Call: After obtaining search results, the function constructs the prompt for the LLM. We may use the Chat Completion API with a system message like: “You are a helpful assistant. Use the information provided to answer the question.” then a message containing the gathered text (e.g., a concatenation of top 3 snippets) and the question. Alternatively, we can format the prompt as a single text block for the Completion API. Once formatted, the function calls the OpenAI service’s endpoint (which is an HTTPS URL specific to our resource and deployed model, e.g., https://<resource-name>.openai.azure.com/openai/deployments/<model>/chat/completions?api-version=2023-05-15). It includes the API key in the header. The body contains the prompt and any parameters (max tokens for answer, temperature, etc.). The OpenAI API returns a JSON with the model’s reply. The function will extract the reply text.
	•	Cosmos DB Storage: The function uses Cosmos DB SDK or Azure Functions binding to record the Q&A. With binding, one can configure the function to automatically write an item to Cosmos when the function ends. Alternatively, manually: prepare a JSON document like {"id": "<uuid>", "sessionId": "...", "question": "...", "answer": "...", "timestamp": "...", "sources": [ ... ]} and use the Cosmos client to insert it into the container. Similarly, for retrieving history, before calling Search/LLM the function can query Cosmos for previous messages in this session. A simple approach: store each session’s conversation as an array in one document (append new QA each time). But that could get large. Instead, storing each turn as separate documents and querying by sessionId for the last N is straightforward (using a query like SELECT * FROM c WHERE c.sessionId = 'abc' ORDER BY c.timestamp DESC LIMIT 5). Cosmos returns JSON results which we can then parse and include relevant past user questions or assistant answers in the prompt if needed. In this initial version, we might only incorporate the immediate last question-answer for context to keep it simple.
	•	Data Formats: The data going into and out of these services are JSON or plain text. Azure Search returns JSON; OpenAI expects and returns JSON; Cosmos deals with JSON. This makes integration easier in a JavaScript/TypeScript or Python Azure Function where JSON can be handled as native objects.
	•	Error Handling: The function will include error handling for each call. If Search returns no results, we might decide to still call the LLM with just the question (the LLM might still attempt an answer from its training data, but that can increase hallucination – another approach is to respond “I found nothing on Wikipedia about that”). If the OpenAI call fails (e.g., rate limit or error), the function should catch that and perhaps retry once or return an error message to the frontend. Cosmos DB errors (on write) would be logged but not critical for responding to the user (the chat can continue even if a log write fails, in a pinch).

Interfaces between Azure Services: The Azure services themselves are loosely coupled through the orchestrator logic:
	•	Azure Search index has no direct knowledge of Cosmos or OpenAI. It just exposes a search API.
	•	Azure OpenAI service doesn’t call Search; it only responds to prompts given by the function.
	•	Cosmos DB is used by the function as needed for storage; it doesn’t trigger or call other parts in this design (although Azure Functions could be triggered by Cosmos DB changes in other scenarios, here we use HTTP trigger).
	•	This design follows a clear separation: retrieve -> then generate.

High-Level Sequence (Summary):
	1.	User enters query in UI, hits “Send”.
	2.	Frontend (React) sends POST /api/ask with JSON payload.
	3.	Azure Function (AskQuestion) receives request. It checks for sessionId; if not present, creates one (e.g., using uuid()).
	4.	The function fetches recent history from Cosmos DB (if any) for that session.
	5.	It then either calls OpenAI embedding (for vector) or directly queries Azure Search with the user question. Suppose we do vector search: it calls embedding API (gets vector), then calls Search with that vector. Azure Search returns, say, 3 relevant passages from Wikipedia.
	6.	The function builds the prompt including the retrieved text and the user’s question (and possibly a brief summary of context from history if multi-turn).
	7.	Calls Azure OpenAI’s completion API with the prompt. Gets the response text.
	8.	Saves the interaction to Cosmos DB (sessionId, question, answer, sources).
	9.	Sends back the answer (and sources, sessionId) to the frontend.
	10.	Frontend displays the answer. The user can then ask another question (which goes with the same sessionId to continue context).

This entire flow is initiated by the frontend and completes typically within a couple of seconds (most time spent in the OpenAI model responding, as Search and DB operations are fast).

API Security and Keys: We will ensure the interfaces are secured appropriately:
	•	The Azure Function endpoint can have an API key (function key) or be publicly accessible. For simplicity in a demo, we might allow anonymous access (since it’s just Q&A and not modifying sensitive data), but in a real scenario, we could enforce an API key or JWT auth (especially if we had user accounts). We assume the environment is controlled enough for now.
	•	All Azure service keys (Search admin key, OpenAI API key, Cosmos key) are kept in Azure Functions application settings (not exposed to client). The function code reads them from environment variables. This way, the frontend never sees any secret.
	•	Azure Cosmos DB can be set with firewall to only allow Azure Function’s outbound IP or have “allow access from Azure Services” enabled, so external parties can’t hit it.
	•	We may also implement CORS on the Function to only allow the React app’s domain to call it if hosting the frontend separately.

Interface to Data Ingestion: (If applicable) We might have a separate process to ingest Wikipedia data to Azure Search. This could be a one-time script run locally or another Azure Function (timer-triggered). That ingestion process would interface with Wikipedia (reading files or API) and Azure Search (index API). While not part of the end-user functionality, it’s a piece of the overall system. For the PRD, suffice it to say we’ll either run a script or use Azure Cognitive Search Indexer to load the data (as described in Data Preparation).

By designing clean interfaces (REST for frontend-backend, and SDK/HTTP for backend to Azure services), we keep the system modular. Each piece can potentially be replaced or scaled independently. For instance, we could swap out the React app with a different frontend (even a console app) as long as it sends the proper API requests. Or we could later replace Azure Functions with a different web framework – the Azure Search and OpenAI calls would remain the same.

6. Implementation Flow & Order of Execution

To build this RAG-enabled chatbot efficiently in 1-2 days, we will follow a focused, step-by-step implementation plan. The idea is to prioritize core functionality first, then add enhancements like chat history and deployment automation. Below is the recommended order of execution with a breakdown of tasks:

Day 1: Backend and Search Index Setup
	1.	Provision Azure Resources: Start by creating the required Azure services:
	•	Set up an Azure Cognitive Search service (if using Free tier, ensure no existing free service in the subscription; otherwise Basic tier). Note the service name and get admin key.
	•	Set up an Azure OpenAI service (if not already available, this requires approval usually – assuming we have access). Deploy the chosen model (e.g., GPT-3.5 Turbo) in that resource and note the deployment name and API key.
	•	Create an Azure Cosmos DB account (use Core (SQL) API). Within it, create a database and container for chat history (partition key = sessionId).
	•	Create an Azure Function App (running your preferred language runtime, e.g., Node.js 18 or Python 3.10). This will host the backend function. Alternatively, this can be done later via deployment scripts; but having the function app ready helps for testing quickly.
	•	(Optional) Prepare a storage account if planning to use it for data import or for hosting the frontend later. Also, ensure you have an Azure account with the ability to use these services within quotas.
	2.	Acquire and Prepare Wikipedia Data: In parallel to provisioning (or after), start working on data:
	•	If using a Wikipedia dump, download a recent dump (this could be time-consuming depending on internet speed; if too slow, pick a smaller subset or a specific category’s pages).
	•	Write a quick parser to extract plain text from the dump. There are Python libraries like mwparserfromhell or existing scripts that can strip markup. Given the time constraint, identify a workable approach (even regex-based cleaning for major markup could suffice for a prototype).
	•	As a shortcut, consider using an available dataset (for example, Wikipedia’s abstract dump or a pre-parsed JSON). If found, transform it into our chunk format.
	•	Segment the text into chunks as discussed. This could be done in code and stored in a list or a JSON lines file.
	•	Prioritization: If time is short, select, say, 500-1000 important articles to include (maybe based on a list of common topics or a certain domain). It’s better to have a smaller working index than an incomplete attempt at the entire Wikipedia.
	3.	Build the Search Index: Once some data is ready:
	•	Define the index schema in Azure Cognitive Search. This can be done via the Azure Portal (Import data wizard) or via a script using the REST API. A quick method: Use the portal’s “Import data” workflow, point it to a JSON file or Azure blob with sample data to infer schema, then adjust as needed. However, to include vector fields, the portal might not support configuring that fully yet, so using the REST API/SDK with an index definition JSON might be required. We will define the content field as searchable text, and add a content_vector of type “Collection(Edm.Single)” with dimensions matching the embedding.
	•	If using code, prepare a small program (could be a notebook or script) to create the index. Azure SDK for Python or JavaScript can do this. This step should yield an empty index with the right fields.
	•	Index the data: Again, using either:
	•	Azure Search Indexer: For example, upload the chunk documents to a Blob container, then configure a data source and indexer to pull them in. This can be done through the portal by uploading a JSON and using “Data Import” wizard.
	•	Or use a script to send index upload batches. The SDK allows pushing 1,000 documents per batch. Loop through chunks and push them. Also, at this time, generate embeddings for each chunk. Ideally, use the Azure OpenAI embedding endpoint in batches (OpenAI embedding API can accept batch of texts, up to e.g. 16 or more at once). This requires writing a script that reads each chunk text, calls embedding API, gets vector, then immediately sends the document to Azure Search with that vector.
	•	Since time is limited, consider indexing without vectors initially (just rely on text search/semantic rank, which Azure Search can do out-of-the-box). This removes the need to generate all embeddings upfront. We can later add vector search if time permits or if needed for better accuracy.
	•	Verify the index has documents by searching in the portal or via a simple query. If the indexer approach is used, check the Indexer execution status for errors.
	•	(Milestone): By mid Day 1, ideally the Wikipedia search index is up and running with at least some data. This is a major dependency for the QA functionality.
	4.	Implement Azure Function (Q&A Endpoint): Begin coding the Azure Function that will serve as the Q&A API.
	•	Choose the language (JavaScript/TypeScript for Node, or Python, or C#). Many Azure samples use Python or C#. Given React frontend, Node/TypeScript might be convenient for using the same language on both ends.
	•	Inside the function, implement the logic to handle a request. Initially, focus on single-turn Q&A (no history) to keep it simple.
	•	Write code to parse the incoming JSON (question etc.), call Azure Search:
	•	If doing keyword search: use Azure Search REST API with an HTTP library (e.g., axios in Node) to GET /indexes/{indexName}/docs?api-version=...&search={query} with query url-encoded. Or use the official SDK which might simplify it.
	•	If doing vector search: call embedding first. For embedding, use the OpenAI SDK or REST call. The OpenAI SDK for Node (or Python) can be configured with the Azure endpoint by setting api-base, etc. Alternatively, use direct REST with fetch/requests. Once embedding is obtained, call Azure Search REST with a POST JSON body: { "vectors": [ { "value": <the vector>, "fields": "content_vector", "k": 3 } ], "search": "<user query>" } if using hybrid (vector + text) or the newer syntax. (We’ll consult Azure Search docs for exact JSON format for vector queries.)
	•	Parse the search results, extract the top passages text.
	•	Call Azure OpenAI to get answer. Construct prompt: we might do something simple like: "Context:\n" + snippet1 + “\n” + snippet2 + … + "\nQuestion: ${userQuestion}\nAnswer:". If using ChatCompletion, set system/message accordingly.
	•	Receive answer, wrap in JSON and return.
	•	Test locally: Azure Functions Core Tools can run the function locally. We can simulate a query to see if it flows. If we don’t have the whole pipeline working yet (e.g., maybe embeddings not set up), test by using a known small set of data or stub the search step by returning a hardcoded context to see that OpenAI call works.
	5.	Frontend Basic Setup: While backend is being coded, also scaffold the React app (if not already). Using create-react-app or Vite to quickly set up a project. Implement a simple UI with:
	•	An input box and submit button (or hitting Enter sends the question).
	•	A conversation display area where we append our question and the bot’s answer.
	•	Optionally, a way to start a new chat (clear context) which would request a new session ID from backend.
	•	For now, the frontend can just call the local dev function or a test endpoint. We ensure CORS is handled (the function app can allow localhost:3000 if needed for dev).
	•	This is relatively quick using existing chat UI component libraries or custom minimal JSX.
	6.	Integrate Frontend with Backend (dev environment): Configure the React app to call the Azure Function API. In development, if running Azure Functions locally on say port 7071 and React on 3000, handle any CORS issues. Once the call works, test the end-to-end: type a question in UI, see if the answer is displayed. At this point, even if the answer is nonsense or empty (depending if search or openAI is returning something), the plumbing being connected is a major step.

Day 2: Enhancements and Deployment
	7.	Implement Chat History (Multiturn): Now that single Q&A works, extend the backend to handle context:
	•	In the Azure Function, incorporate Cosmos DB. Use the Azure SDK to connect to Cosmos (with the endpoint and key). Node SDK, for example, can be used to upsert items.
	•	Before answering, fetch last one or two Q&A from Cosmos for the session. The simplest approach: on receiving a question, query Cosmos for all items with that sessionId, order by timestamp descending, and take the top 1 or 2. Then include the previous Q&A in the prompt (e.g., “Previous Q: …, Previous A: …” or as part of system message).
	•	After generating the answer, write the new question and answer to Cosmos. We ensure to include sessionId and maybe an index or time.
	•	Update the API to handle sessionId properly: if the request had no sessionId, generate one and return it. The frontend should then store this (perhaps in a React state or in localStorage if we want persistence across page reloads).
	•	Test multi-turn manually: Ask a question, then a follow-up that depends on the first. Check if the answer improves when history is included versus not. Tweak prompt formatting if needed for clarity to the model (sometimes prefixing the conversation history as part of system or user message helps).
	•	The acceptance criteria for context (from the user stories) can be tested now.
	8.	Refine Answer Formatting: If we want the chatbot to cite sources or provide the article title in the answer, we may adjust the prompt given to the LLM. For example, we could instruct the model: “Include the source article title in parentheses for any fact you use.” Alternatively, since we have the source list from search results, a simpler approach is to not rely on the model to cite, but have our function attach a list of source titles in the response JSON. The React UI can then display something like “Sources: [Wikipedia: ArticleName]” below the answer. This is an implementation detail, but we will do a quick pass to make the output user-friendly and trustworthy.
	•	For now, we will include the sources array in the JSON (with maybe the top 2 Wikipedia article titles) for transparency. The UI can show them.
	•	Keep the answer text itself clean (the model’s natural answer).
	9.	GitHub Actions – CI/CD Setup: With core functionality in place, set up automation to deploy this project to Azure:
	•	Repository Initialization: Ensure all code (frontend and backend) is in a GitHub repository. The project likely has two parts: a React app and an Azure Function app. We might keep them in one repository for simplicity (e.g., two folders: frontend/ and backend/).
	•	Workflow for Azure Functions: Create a GitHub Actions workflow YAML (e.g., .github/workflows/deploy-backend.yml). Use the official Azure Functions action ￼ to login to Azure (using service principal credentials or OIDC), then build and deploy the function. This typically involves:
	•	Checkout code.
	•	Setup the correct language environment (install Node if Node function, etc.).
	•	Zip or prepare the function package.
	•	Use azure/functions-action@v1 to deploy to the function app (needs function app name, Azure creds, etc.).
This automation means whenever we push changes to the backend code (e.g., main branch), the function gets updated in Azure. This satisfies continuous deployment ￼.
	•	Workflow for Frontend: If deploying the React app to a static site:
	•	We can use Azure Static Web Apps for an easy route – it has its own GitHub Action that will build the React app and publish it. If we created a Static Web App resource, we’d use the Azure/static-web-apps-deploy action with appropriate config (app location, build output path). This also can automatically manage the backend if it’s an Azure Function in the same repo (but since we already separately deployed the function, we might just host the frontend here).
	•	Alternatively, deploy React to a blob storage static website or to an Azure App Service. If Blob, a GH Action can run the React build (npm run build) and then use Azure CLI to upload the static files to blob storage. If App Service, use azure/webapps-deploy action to push the build output.
	•	Document which approach is chosen. The key is an automated pipeline from repo to Azure hosting for the frontend as well.
	•	Infrastructure as Code: Since time is short, manual creation of resources is fine, but if possible, include an Azure Developer CLI (azd) template or bicep file in the repo. This way, setting up a new environment is faster. Notably, the Azure-Samples and GPT-RAG projects use azd for quick provisioning ￼, which in one command sets up resources and even does initial deployment.
	•	Testing the pipeline: After pushing the workflows, verify that the backend is reachable via its Azure URL and the fronted is served (the static web app’s domain or the published site).
	10.	Final Testing & Tuning: Deploy the latest code to Azure via the actions. Then test the live system at its public endpoints:
	•	Open the deployed React app (or if not using a separate site, use a tool like Postman to hit the function URL directly) and simulate real user queries.
	•	Test a variety of questions, including those with follow-ups, to ensure everything works in the cloud environment. Sometimes differences (like Azure Functions configuration or memory limits) might surface issues to fix (e.g., maybe the function needs a higher timeout if OpenAI calls take long).
	•	Monitor logs in Azure (via Functions log stream or App Insights if configured) to see if any errors occur (for example, OpenAI errors or Cosmos DB connectivity issues). Fix any minor issues found.
	•	Evaluate answer quality. If answers are not accurate enough, consider enabling semantic search on Azure Search (which can be done by enabling the semantic ranking in the query, if using certain tiers) to improve document relevance. Or increase the number of documents retrieved and allow the model more context.
	•	Ensure that the system meets the acceptance criteria defined (for accuracy, context, etc.). If, for instance, the model is hallucinating or not using the provided context properly, we may adjust the prompt (e.g., make the system message very explicit: “Only answer using the given content. If you don’t find the answer in it, say you don’t know.”).
	•	Check performance: measure roughly how long a query takes. If too slow (say >5s consistently), see where the bottleneck is (embedding step, search, or OpenAI). Possibly reduce number of retrieved docs or shorten the prompt to speed up OpenAI’s response, or switch to smaller model if needed.
	11.	Documentation & Handoff: (If this were a real project handoff.) Document how to run and deploy the system, any remaining to-dos, and how to extend it. In our case, ensure the PRD and possibly a README in the repo explains usage.

Prioritized Task Breakdown:
	•	Highest Priority: Basic Q&A pipeline (Steps 2, 3, 4) – because without this, nothing else matters. Have a working search and LLM integration as soon as possible.
	•	Next: Frontend integration (Step 5, part of 6) – so we can demo end-to-end and catch any integration issues early.
	•	Then: Multi-turn enhancements (Step 7) – adds value but the system can function even without it initially.
	•	Then: Deployment automation (Step 9) – to save time and ensure we meet the “quick deployment” goal. However, if time is almost over, manual deployment can temporarily suffice and CI/CD can be done after demo.
	•	Finally: Polish, testing, performance tuning (Step 10).

At each stage, have a running version (even if partially featured). For example, after Step 4 we should be able to answer questions via a REST client. After Step 6, we should be able to do it via the actual UI. This iterative build ensures that even if we run out of time, we have a demoable chatbot that answers questions (perhaps without history).

GitHub Actions for Automation: As described, we plan to include workflows for CI/CD:
	•	A backend deployment workflow triggered on pushes to main or a specific path (e.g., backend/*). It will use Azure login, then deploy the Azure Function using either the Azure Functions action or Azure CLI scripts. The key is automating the build and release of the function app so that code changes go live without manual intervention ￼.
	•	A frontend deployment workflow triggered on pushes to the frontend code. If using Azure Static Web Apps, that service integrates with its own GitHub Action that we configure (through the SWA creation process) – it will build the React app and deploy it. If using an App Service or blob, we will use the respective actions (Azure Web App deploy or Azure CLI).
	•	Optionally, an infrastructure workflow – if we write Bicep or use azd, we can have a pipeline to deploy infrastructure. Given the short project timeline, this might be manual or one-time, but it’s good to keep the config files (like a Bicep or Terraform) in source control for reproducibility.

These actions will help achieve continuous delivery and make it easy to iterate on the project beyond the initial 1-2 day build. By the end of Day 2, we expect to have all components deployed and the chatbot working end-to-end in Azure.

7. Deployment & Scaling Considerations

Deployment Strategy: We will deploy all components to Azure in a manner that is easy to manage and cost-effective:
	•	Azure Functions (Backend): The function app will be deployed to Azure on a Consumption Plan (serverless). This means we don’t have to manage servers – Azure will run the function on demand and scale out as needed. Deployment of the function code can be done via the GitHub Actions pipeline (or manually via VS Code/Azure CLI if needed). After deployment, we’ll have an endpoint (like https://<functionapp>.azurewebsites.net/api/ask) that the frontend can call. We should configure application settings on the function app for the Azure Search service name, search index name, search API key, OpenAI endpoint & key, and Cosmos DB credentials. These can be set in Azure Portal or via scripts. Once deployed, testing the function with an HTTP request in the portal or using curl will ensure it’s running properly.
	•	React Frontend: For hosting the React app, a recommended approach is Azure Static Web Apps or Azure Blob Storage static site:
	•	Static Web App: This service automatically provides a global endpoint for the React app and can also integrate with an Azure Function backend (linking them). If we use this, deploying is very straightforward: the GH Action will publish the app, and SWA will handle serving and even set up a domain. It also has free tier for low usage. Since we already have a function app separately, we might just host the React build output on Static Web Apps or even on GitHub Pages as an alternative. However, Static Web Apps give the benefit of being in Azure’s domain and close to our other resources.
	•	Blob Storage: We can enable static website hosting on an Azure Storage account and upload the build directory (HTML, JS, CSS files) there. Then the site is accessible via the storage endpoint (and can map a custom domain if needed). This is inexpensive (pennies for storage and data egress).
	•	Either way, ensure the frontend is configured to call the correct backend URL (maybe put it in a config file or environment variable during build).
	•	We will also consider CORS – if using a separate domain for frontend, the Azure Function should allow that origin. In Static Web Apps, it can proxy to functions seamlessly if set up in same SWA resource.
	•	Azure Cognitive Search Index: The search index exists within the Azure Cognitive Search service. We don’t “deploy” it like code, but we ensure it’s populated with data as described. If we used an indexer connected to blob/cosmos, that indexer can be scheduled or run on demand to refresh data. For deployment considerations: Azure Search on Free tier has limits (3 indexes, max document count ~50k, etc.). If our Wikipedia subset is larger, we’ll use at least the Basic tier (which allows more storage and up to 1 million docs per partition). Basic tier has one partition and replica by default. This should be fine for a demo scale. We can create the index via scripts or portal – once created and loaded, it’s persistent. We might output the index definition (schema) and keep a copy in the repository for reference. Backup plan: if data ingestion takes too long or is incomplete, the chatbot will still function (the LLM could answer from its training data, but that defeats the purpose; however, minimal data is better than none).
	•	Azure OpenAI Service: Deployment here is essentially selecting the model and deployment name in the Azure OpenAI Studio. We will use the model endpoint as configured. One consideration: rate limits. For GPT-3.5, we might get around 100 requests/minute limit by default (varies by region/quota). This is fine for small-scale usage. We should monitor usage to avoid running up cost. If we expect a lot of queries, we might want to set max tokens for answers wisely (maybe limit to 300 tokens per answer) to control consumption. Since Azure OpenAI is pay-per-token, each answer’s cost = (tokens in prompt + tokens in answer) * model rate. Using retrieval will keep prompt length moderate (few passages + question), likely within a few hundred tokens, and answer few hundred tokens, so cost per question might be on the order of fractions of a cent for GPT-3.5.
	•	Azure Cosmos DB: We’ll deploy Cosmos DB in the cloud with a container for chat history. For small scale, we can use the serverless or autoscale mode so we don’t have to pre-provision a lot of throughput. Serverless lets us pay per operation (good for dev/test with intermittent use) up to certain limits. If using a provisioned throughput, we might set it low (e.g., 400 RU/s) which is the minimum and should handle our light load easily. Another cost-saving: enable the free-tier discount on the Cosmos account if available (gives 400 RU free).
	•	The container should have a partition key (sessionId) so that retrieving by session is efficient. This also allows scaling if many sessions because each session’s data can be fetched in one partition.
	•	Deployment of Cosmos resources can be done via Azure CLI or portal; since it’s mostly a one-time setup, doing it in portal or with a quick script is fine.
	•	Scalability Considerations: Our design is cloud-native and can scale to a point:
	•	Azure Functions: On the consumption plan, will scale out horizontally if many concurrent requests come in. Each instance can handle requests (with some limits on concurrency per instance). If expecting higher load, we could switch to a Premium plan or increase max instances. But for demo, consumption is enough and cost-efficient (we pay only per execution and resource time).
	•	Azure Search: If query volume increases or the dataset increases, we might need to add replicas for more query throughput, or partitions for more storage. For now, a single partition & replica is fine. Query latency is low (tens of ms) for even moderately large indexes, so the bottleneck is more likely the LLM. Azure Search can handle scaling internally if needed (we could up it to 2 replicas for better performance under load).
	•	Azure OpenAI: The service has rate limits per instance. If we needed to scale beyond that, we’d possibly need to request quota increase or spin up another OpenAI resource (not typical). But because each question goes to the model sequentially, the biggest impact on user experience is the response time of the model. We can reduce the temperature (for more deterministic answers) and moderate length to get slightly faster responses. If we had many concurrent users, we’d have to queue or accept that some calls are parallel. The function can handle parallel calls, but the OpenAI service might queue internally if hitting throughput limits.
	•	Cosmos DB: Can scale massively if needed by increasing RU/s or enabling auto-scale. Our usage (a few writes and reads per question) is minimal. With proper partitioning, it can handle far more than we’ll throw at it. So no issues here.
	•	Cost Efficiency Tips:
	•	Use Free tiers and small instances where possible. For example, Azure Search Free if data allows (though with Wikipedia, likely too large for free tier, so Basic tier is the next cheapest). Azure Functions consumption has a generous free grant of execution time per month. Cosmos free tier or serverless to avoid charges when idle. Azure Static Web Apps has a free level for low bandwidth.
	•	Prefer GPT-3.5 over GPT-4 to drastically cut costs (GPT-4 is ~15x more expensive per token). Only use GPT-4 if a question absolutely needs deeper reasoning and if enabled for the resource.
	•	Batch operations during data ingestion (embedding multiple texts in one API call, uploading multiple docs in one index batch) to reduce overhead and cost.
	•	Monitor usage: We can set up Azure cost alerts if needed or just periodically check the metrics (OpenAI has usage charts, Cosmos shows RU consumption, etc.).
	•	Since this is a demo/small-scale, we won’t need multiple regions or redundancy – all services can be in one region (preferably where OpenAI is available and possibly close to us for latency, e.g., East US or West Europe). This avoids inter-region data transfer costs and simplifies configuration.
	•	Potential Future Scaling: If this project were to evolve, considerations might include:
	•	Adding Azure API Management in front of the function for better security, caching, and request throttling (the architecture diagram we showed includes APIM as an optional layer). APIM could also handle auth (like requiring a subscription key or JWT).
	•	Adding a caching layer (like Azure Cache for Redis) to store recent queries and answers in memory. This could speed up responses for repeated questions by bypassing the OpenAI call if we have a cached answer. Our design with Cosmos storing embeddings could be leveraged for this too by doing a similarity search on past Q’s ￼.
	•	Using a dedicated vector database (if not Azure Search) — though Azure Search is sufficient, some might consider using Pinecone or Azure Cosmos DB’s native vector capabilities if extremely large vector sets or specific features are needed ￼. Cosmos DB is rolling out vector indexing in preview ￼, which could unify vector storage with chat history, but for now Azure Search covers our needs.
	•	Finetuning or expanding to multiple knowledge sources (the architecture can support plugging in other data, e.g., documentation, by indexing them too or using multiple indexes).

Deployment Steps Recap: Finally, how to deploy each component:
	•	Use the GitHub Actions (CI/CD) to deploy code (Function and React app) from the repository to Azure.
	•	Use a script or manual steps to deploy the Search index and load data (since that might not be triggered via CI/CD; it might be a one-off or a separate process).
	•	Verify each service: e.g., test a search query via Azure portal search explorer, test OpenAI by a simple prompt in Azure OpenAI Studio, test Cosmos by writing a sample item.
	•	Then test the integrated pipeline as a whole through the frontend.

By following this PRD’s guidance, the development team should be able to implement the RAG-enabled Azure chatbot in a short time and deploy it successfully. The architecture ensures that even at small scale, the system is robust and can be scaled up in the future. Overall, this project demonstrates how Azure’s AI and search capabilities can be combined to create a powerful QA chatbot using the vast knowledge of Wikipedia, within a modern cloud application architecture.

Sources:
	1.	Microsoft Learn – Retrieval Augmented Generation (RAG) in Azure AI Search ￼ ￼ ￼
	2.	Wikipedia – Retrieval-augmented generation (RAG) ￼ (concept overview)
	3.	Azure Integrations Blog – Technical Architecture for Azure OpenAI and Cognitive Search (diagram and workflow) ￼ ￼
	4.	Azure OpenAI Documentation – Using your data with Azure OpenAI (embedding and prompt flow) ￼ ￼
	5.	Advancing Analytics – Unlocking Semantic Logging with CosmosDB (Cosmos DB for chat history caching) ￼ ￼
	6.	Stack Overflow – How to download and work with Wikipedia data dumps (data source options) ￼
	7.	Azure-Samples – azure-search-openai-demo (reference implementation for chat with Azure Search & OpenAI) ￼
	8.	Microsoft Learn – Continuous delivery by using GitHub Actions (Azure Functions) ￼